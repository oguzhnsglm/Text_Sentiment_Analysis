{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0                     Stuning even for the non-gamer   \n",
      "1              The best soundtrack ever to anything.   \n",
      "2                                           Amazing!   \n",
      "3                               Excellent Soundtrack   \n",
      "4  Remember, Pull Your Jaw Off The Floor After He...   \n",
      "\n",
      "                                                text  label  \n",
      "0  This sound track was beautiful! It paints the ...      2  \n",
      "1  I'm reading a lot of reviews saying that this ...      2  \n",
      "2  This soundtrack is my favorite music of all ti...      2  \n",
      "3  I truly like this soundtrack and I enjoy video...      2  \n",
      "4  If you've played the game, you know how divine...      2  \n",
      "\n",
      "Veri seti boyutu: (4000000, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import bz2\n",
    "\n",
    "# BZ2 uzantÄ±lÄ± dosyalarÄ± satÄ±r satÄ±r okuyan fonksiyon\n",
    "def parse_bz2_data(file_path):\n",
    "    data = []\n",
    "\n",
    "    with bz2.open(file_path, 'rt', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            label, text = line.strip().split(' ', 1)\n",
    "            title, text = text.split(': ', 1)\n",
    "            label = int(label.replace('__label__', ''))\n",
    "            data.append((title.strip(), text.strip(), label))\n",
    "    \n",
    "    return pd.DataFrame(data, columns=['title', 'text', 'label'])\n",
    "\n",
    "# ðŸ” Dosya yollarÄ± (senin gÃ¶nderdiÄŸin dosya adlarÄ± ile)\n",
    "train_path = \"train.ft.txt.bz2\"\n",
    "test_path = \"test.ft.txt.bz2\"\n",
    "\n",
    "# âœ… Verileri oku\n",
    "df_train = parse_bz2_data(train_path)\n",
    "df_test = parse_bz2_data(test_path)\n",
    "\n",
    "# ðŸ”— TÃ¼mÃ¼nÃ¼ birleÅŸtir\n",
    "df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "# ðŸŽ¯ GÃ¶z at\n",
    "print(df.head())\n",
    "print(\"\\nVeri seti boyutu:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamazon_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"amazon_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sagla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sagla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re, string\n",
    "import nltk\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Sadece gerekli olan NLTK kaynaklarÄ±nÄ± indiriyoruz\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri kÃ¼mesi yÃ¼klendi.\n",
      "   Unnamed: 0                                              title  \\\n",
      "0           0                     Stuning even for the non-gamer   \n",
      "1           1              The best soundtrack ever to anything.   \n",
      "2           2                                           Amazing!   \n",
      "3           3                               Excellent Soundtrack   \n",
      "4           4  Remember, Pull Your Jaw Off The Floor After He...   \n",
      "\n",
      "                                                text  label  \n",
      "0  This sound track was beautiful! It paints the ...      2  \n",
      "1  I'm reading a lot of reviews saying that this ...      2  \n",
      "2  This soundtrack is my favorite music of all ti...      2  \n",
      "3  I truly like this soundtrack and I enjoy video...      2  \n",
      "4  If you've played the game, you know how divine...      2  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# CSV dosyasÄ±nÄ± oku (aynÄ± klasÃ¶rde olduÄŸunu varsayalÄ±m)\n",
    "df = pd.read_csv('amazon_dataset.csv')\n",
    "print(\"Veri kÃ¼mesi yÃ¼klendi.\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  label\n",
      "0  Stuning even for the non-gamer This sound trac...      2\n",
      "1  The best soundtrack ever to anything. I'm read...      2\n",
      "2  Amazing! This soundtrack is my favorite music ...      2\n",
      "3  Excellent Soundtrack I truly like this soundtr...      2\n",
      "4  Remember, Pull Your Jaw Off The Floor After He...      2\n"
     ]
    }
   ],
   "source": [
    "# title ve text alanlarÄ±nÄ± birleÅŸtir (araya boÅŸluk koyarak)\n",
    "df['review'] = df['title'] + ' ' + df['text']\n",
    "\n",
    "# Kontrol et\n",
    "print(df[['review', 'label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_basic(text):\n",
    "    # KÃ¼Ã§Ã¼k harf yap\n",
    "    text = text.lower()\n",
    "    \n",
    "    # HTML etiketlerini kaldÄ±r\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # URL'leri kaldÄ±r\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Noktalama iÅŸaretlerini kaldÄ±r\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Emoji temizleme\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\" u\"\\U0001F600-\\U0001F64F\" u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\" u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002702-\\U000027B0\" \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sadece string olan ve NaN olmayan deÄŸerleri al\n",
    "df = df[df['review'].apply(lambda x: isinstance(x, str))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# POS bilgisi olmadan varsayÄ±lan isim (noun) kabul ederek sade lemmatization\n",
    "def lemmatize_simple(text):\n",
    "    words = text.split()  # burada tokenizer yerine split kullanÄ±yoruz\n",
    "    cleaned = [\n",
    "        lemmatizer.lemmatize(word) for word in words\n",
    "        if word.lower() not in stop_words and word.isalpha()\n",
    "    ]\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "# Full temizlik + lemmatizasyon\n",
    "def full_clean(text):\n",
    "    text = clean_text_basic(text)\n",
    "    text = lemmatize_simple(text)\n",
    "    return text\n",
    "\n",
    "# Uygula\n",
    "df['cleaned_review'] = df['review'].apply(full_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = {\"omg\": \"oh my god\", \"lol\": \"laugh out loud\", \"u\": \"you\", \"r\": \"are\", \"gr8\": \"great\", \"b4\": \"before\", \"asap\": \"as soon as possible\", \"btw\": \"\", \"fyi\": \"for your information\", \"idk\": \"i don't know\", \"imo\": \"in my opinion\", \"jk\": \"just kidding\", \"lmao\": \"laughing my ass off\", \"lmk\": \"let me know\", \"nvm\": \"nevermind\", \"np\": \"no problem\", \"rofl\": \"rolling on the floor laughing\", \"smh\": \"shaking my head\", \"tbh\": \"to be honest\", \"thx\": \"thanks\", \"ty\": \"thank you\", \"wth\": \"what the heck\", \"wtf\": \"what the f***\", \"yolo\": \"you only live once\", \"brb\": \"be right back\", \"gtg\": \"got to go\", \"btw\": \"by the way\", \"ttyl\": \"talk to you later\", \"ily\": \"I love you\", \"g2g\": \"got to go\", \"h8\": \"hate\", \"jk\": \"just kidding\", \"thx\": \"thanks\", \"ttyl\": \"talk to you later\", \"cya\": \"see you\", \"gg\": \"good game\", \"afk\": \"away from keyboard\", \"ez\": \"easy\", \"wb\": \"welcome back\", \"idc\": \"i don't care\", \"rn\": \"right now\", \"lmk\": \"let me know\", \"ikr\": \"i know right\", \"tmi\": \"too much information\", \"smh\": \"shaking my head\", \"w/e\": \"whatever\", \n",
    "                 \"bff\": \"best friends forever\", \"tfw\": \"that feeling when\", \"np\": \"no problem\", \"nvm\": \"nevermind\", \"fyi\": \"for your information\", \"cuz\": \"because\", \"gonna\": \"going to\", \"wanna\": \"want to\", \"gotta\": \"got to\", \"kinda\": \"kind of\", \"sorta\": \"sort of\", \"dunno\": \"don't know\", \"ain't\": \"is not\", \"gimme\": \"give me\", \"lemme\": \"let me\", \"im\": \"I'm\", \"hes\": \"he's\", \"shes\": \"she's\", \"theyre\": \"they're\", \"youre\": \"you're\", \"havent\": \"have not\", \"cant\": \"cannot\",\n",
    "                  \"couldnt\": \"could not\", \"didnt\": \"did not\", \"doesnt\": \"does not\", \"dont\": \"do not\", \"isnt\": \"is not\", \"mightnt\": \"might not\", \"mustnt\": \"must not\", \"shouldnt\": \"should not\", \"wasnt\": \"was not\", \"werent\": \"were not\", \"wouldnt\": \"would not\", \"tho\": \"though\", \"thru\": \"through\", \"nite\": \"night\", \"neva\": \"never\", \"sum\": \"some\", \"dat\": \"that\", \"dis\": \"this\", \"dem\": \"them\", \"dey\": \"they\", \"em\": \"them\", \"whered\": \"where did\", \"whod\": \"who did\", \"whos\": \"who's\", \"yall\": \"you all\", \"dam\": \"damn\", \"hell\": \"hell\", \"pissed\": \"pissed\", \"fck\": \"fuck\", \"fuk\": \"fuck\", \"effing\": \"effing\", \"bs\": \"bullshit\", \"crap\": \"crap\", \"shitty\": \"shitty\", \"wtf\": \"what the fuck\", \"stfu\": \"shut the fuck up\", \"gtfo\": \"get the fuck out\", \"irl\": \"in real life\", \"roflmao\": \"rolling on the floor laughing my ass off\", \"smol\": \"small\", \"big oof\": \"huge mistake\", \"yeet\": \"throw something forcefully\", \"pog\": \"play of the game\", \"sus\": \"suspicious\", \"cap\": \"lie\", \"no cap\": \"no lie\", \"bet\": \"okay\", \"lit\": \"amazing\", \"fr\": \"for real\", \"bruh\": \"bro\", \"fam\": \"family\",\n",
    "                    \"goat\": \"greatest of all time\", \"lowkey\": \"somewhat\", \"highkey\": \"very\", \"vibe\": \"mood\", \"drip\": \"fashionable\", \"slay\": \"do something well\", \"tea\": \"gossip\", \"sksksk\": \"excited reaction\", \"tf\": \"the fuck\", \"fomo\": \"fear of missing out\", \"tldr\": \"too long didn't read\", \"hmu\": \"hit me up\", \"wyd\": \"what you doing\", \"wym\": \"what you mean\", \"wdym\": \"what do you mean\", \"btw\": \"by the way\", \"imo\": \"in my opinion\", \"idc\": \"i don't care\", \n",
    "                  \"idgaf\": \"i don't give a f***\", \"frfr\": \"for real for real\", \"mf\": \"motherfucker\", \"rn\": \"right now\", \"lolz\": \"laughs\", \"hmu\": \"hit me up\", \"hbu\": \"how about you\", \"rn\": \"right now\", \"lmk\": \"let me know\", \"ikr\": \"i know right\", \"jk\": \"just kidding\", \"fr\": \"for real\", \"ffs\": \"for fuck's sake\", \"btw\": \"by the way\", \"nfs\": \"not for sale\", \"dm\": \"direct message\", \"af\": \"as fuck\", \"idk\": \"i don't know\"}\n",
    "\n",
    "def correct_slangs(text, slang_dict):\n",
    "    for slang, replacement in slang_dict.items():\n",
    "        text = re.sub(rf'\\b{slang}\\b', replacement, text)\n",
    "    return text\n",
    "\n",
    "# Slang dÃ¼zeltmeyi orijinal review'e uygula (henÃ¼z temizlenmemiÅŸ)\n",
    "df['cleaned_review'] = df['cleaned_review'].apply(lambda x: correct_slangs(x.lower(), slang_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cleaned_amazon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"cleaned_amazon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN olanlarÄ± temizle (her ihtimale karÅŸÄ±)\n",
    "df = df.dropna(subset=['cleaned_review', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri ÅŸekilleri:\n",
      "Train: (3199792,), Test: (799949,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Etiketleri 2 â†’ 1 (positive), 1 â†’ 0 (negative) olacak ÅŸekilde dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "df['label'] = df['label'].map({2: 1, 1: 0})\n",
    "\n",
    "# Ã–zellik ve etiketleri ayÄ±r\n",
    "X = df['cleaned_review'].values\n",
    "y = df['label'].values\n",
    "\n",
    "# %80 train, %20 test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Kontrol\n",
    "print(\"Veri ÅŸekilleri:\")\n",
    "print(f\"Train: {x_train.shape}, Test: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# TfidfVectorizer (Binary + max 5000 feature)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m x_train_vec \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m x_test_vec \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(x_test)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Model eÄŸitme ve deÄŸerlendirme fonksiyonu\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sagla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2090\u001b[0m )\n\u001b[1;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sagla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sagla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sagla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1266\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   1267\u001b[0m         \u001b[38;5;66;03m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1270\u001b[0m \u001b[43mj_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_counter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1271\u001b[0m values\u001b[38;5;241m.\u001b[39mextend(feature_counter\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m   1272\u001b[0m indptr\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(j_indices))\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# TfidfVectorizer (Binary + max 5000 feature)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "x_train_vec = vectorizer.fit_transform(x_train)\n",
    "x_test_vec = vectorizer.transform(x_test)\n",
    "\n",
    "# Model eÄŸitme ve deÄŸerlendirme fonksiyonu\n",
    "def train_and_evaluate_model(model, model_name):\n",
    "    print(f\"\\nðŸ”¹ {model_name}\")\n",
    "    model.fit(x_train_vec, y_train)\n",
    "    preds = model.predict(x_test_vec)\n",
    "    # Skorlar\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Logistic Regression\n",
      "Logistic Regression - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89    401033\n",
      "           1       0.88      0.89      0.89    398913\n",
      "\n",
      "    accuracy                           0.89    799946\n",
      "   macro avg       0.89      0.89      0.89    799946\n",
      "weighted avg       0.89      0.89      0.89    799946\n",
      "\n",
      "Logistic Regression - Accuracy on Training Set: 0.8865\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "train_and_evaluate_model(LogisticRegression(max_iter=1000), \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Multinomial Naive Bayes\n",
      "Multinomial Naive Bayes - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85    401033\n",
      "           1       0.85      0.85      0.85    398913\n",
      "\n",
      "    accuracy                           0.85    799946\n",
      "   macro avg       0.85      0.85      0.85    799946\n",
      "weighted avg       0.85      0.85      0.85    799946\n",
      "\n",
      "Multinomial Naive Bayes - Accuracy on Training Set: 0.8482\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes\n",
    "train_and_evaluate_model(MultinomialNB(), \"Multinomial Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_model(LinearSVC(), \"Linear SVC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "train_and_evaluate_model(RandomForestClassifier(n_estimators=100), \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "train_and_evaluate_model(SVC(), \"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Tokenizer ayarlarÄ±\n",
    "max_words = 10000     # En sÄ±k kullanÄ±lan 10.000 kelime\n",
    "max_len = 200         # CÃ¼mle uzunluÄŸu sabitlenecek\n",
    "\n",
    "# Tokenizer eÄŸitimi\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "# Metinleri dizilere Ã§evir\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Pad (doldurma)\n",
    "x_train_pad = pad_sequences(x_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "x_test_pad = pad_sequences(x_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Etiketleri array yap\n",
    "y_train_pad = np.array(y_train)\n",
    "y_test_pad = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def create_model(model_type='rnn'):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(max_words, 64, input_length=max_len))\n",
    "    \n",
    "    if model_type == 'rnn':\n",
    "        model.add(tf.keras.layers.SimpleRNN(64))\n",
    "    elif model_type == 'lstm':\n",
    "        model.add(tf.keras.layers.LSTM(64))\n",
    "    elif model_type == 'cnn':\n",
    "        model.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "        model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model_type):\n",
    "    print(f\"\\nðŸ“š Training {model_type.upper()} model...\")\n",
    "    model = create_model(model_type)\n",
    "    model.fit(x_train_pad, y_train, epochs=5, batch_size=128, validation_split=0.2, verbose=1)\n",
    "    \n",
    "    # Tahmin\n",
    "    y_pred = (model.predict(x_test_pad) > 0.5).astype(\"int32\")\n",
    "    print(f\"\\nðŸ“Š {model_type.upper()} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ['cnn', 'rnn', 'lstm']:\n",
    "    train_and_evaluate(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
