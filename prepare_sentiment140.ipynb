{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sagla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sagla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sagla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sagla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re, string\n",
    "import nltk\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Sadece gerekli olan NLTK kaynaklarÄ±nÄ± indiriyoruz\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                             review\n",
      "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1          0  is upset that he can't update his Facebook by ...\n",
      "2          0  @Kenichan I dived many times for the ball. Man...\n",
      "3          0    my whole body feels itchy and like its on fire \n",
      "4          0  @nationwideclass no, it's not behaving at all....\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV dosyasÄ±nÄ± yÃ¼kle\n",
    "df = pd.read_csv(\"sentiment140_dataset.csv\", header=None, encoding='latin-1')\n",
    "\n",
    "# Orijinal sÃ¼tunlara geÃ§ici isimler verelim\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'review']\n",
    "\n",
    "# Sadece sentiment ve review sÃ¼tunlarÄ±nÄ± al\n",
    "df = df[['sentiment', 'review']]\n",
    "\n",
    "# sentiment sÃ¼tununu string'den integer'a Ã§evir\n",
    "df['sentiment'] = df['sentiment'].astype(int)\n",
    "\n",
    "# Ä°lk 5 satÄ±rÄ± kontrol et\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_basic(text):\n",
    "    # KÃ¼Ã§Ã¼k harf yap\n",
    "    text = text.lower()\n",
    "    \n",
    "    # HTML etiketlerini kaldÄ±r\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # URL'leri kaldÄ±r\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # ğŸ”¹ MentionlarÄ± kaldÄ±r (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Noktalama iÅŸaretlerini kaldÄ±r\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Emoji temizleme\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\" u\"\\U0001F600-\\U0001F64F\" u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\" u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002702-\\U000027B0\" \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # Fazla boÅŸluklarÄ± temizle\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# POS bilgisi olmadan varsayÄ±lan isim (noun) kabul ederek sade lemmatization\n",
    "def lemmatize_simple(text):\n",
    "    words = text.split()  # burada tokenizer yerine split kullanÄ±yoruz\n",
    "    cleaned = [\n",
    "        lemmatizer.lemmatize(word) for word in words\n",
    "        if word.lower() not in stop_words and word.isalpha()\n",
    "    ]\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "# Full temizlik + lemmatizasyon\n",
    "def full_clean(text):\n",
    "    text = clean_text_basic(text)\n",
    "    text = lemmatize_simple(text)\n",
    "    return text\n",
    "\n",
    "# Uygula\n",
    "df['cleaned_review'] = df['review'].apply(full_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = {\"omg\": \"oh my god\", \"lol\": \"laugh out loud\", \"u\": \"you\", \"r\": \"are\", \"gr8\": \"great\", \"b4\": \"before\", \"asap\": \"as soon as possible\", \"btw\": \"\", \"fyi\": \"for your information\", \"idk\": \"i don't know\", \"imo\": \"in my opinion\", \"jk\": \"just kidding\", \"lmao\": \"laughing my ass off\", \"lmk\": \"let me know\", \"nvm\": \"nevermind\", \"np\": \"no problem\", \"rofl\": \"rolling on the floor laughing\", \"smh\": \"shaking my head\", \"tbh\": \"to be honest\", \"thx\": \"thanks\", \"ty\": \"thank you\", \"wth\": \"what the heck\", \"wtf\": \"what the f***\", \"yolo\": \"you only live once\", \"brb\": \"be right back\", \"gtg\": \"got to go\", \"btw\": \"by the way\", \"ttyl\": \"talk to you later\", \"ily\": \"I love you\", \"g2g\": \"got to go\", \"h8\": \"hate\", \"jk\": \"just kidding\", \"thx\": \"thanks\", \"ttyl\": \"talk to you later\", \"cya\": \"see you\", \"gg\": \"good game\", \"afk\": \"away from keyboard\", \"ez\": \"easy\", \"wb\": \"welcome back\", \"idc\": \"i don't care\", \"rn\": \"right now\", \"lmk\": \"let me know\", \"ikr\": \"i know right\", \"tmi\": \"too much information\", \"smh\": \"shaking my head\", \"w/e\": \"whatever\", \n",
    "                 \"bff\": \"best friends forever\", \"tfw\": \"that feeling when\", \"np\": \"no problem\", \"nvm\": \"nevermind\", \"fyi\": \"for your information\", \"cuz\": \"because\", \"gonna\": \"going to\", \"wanna\": \"want to\", \"gotta\": \"got to\", \"kinda\": \"kind of\", \"sorta\": \"sort of\", \"dunno\": \"don't know\", \"ain't\": \"is not\", \"gimme\": \"give me\", \"lemme\": \"let me\", \"im\": \"I'm\", \"hes\": \"he's\", \"shes\": \"she's\", \"theyre\": \"they're\", \"youre\": \"you're\", \"havent\": \"have not\", \"cant\": \"cannot\",\n",
    "                  \"couldnt\": \"could not\", \"didnt\": \"did not\", \"doesnt\": \"does not\", \"dont\": \"do not\", \"isnt\": \"is not\", \"mightnt\": \"might not\", \"mustnt\": \"must not\", \"shouldnt\": \"should not\", \"wasnt\": \"was not\", \"werent\": \"were not\", \"wouldnt\": \"would not\", \"tho\": \"though\", \"thru\": \"through\", \"nite\": \"night\", \"neva\": \"never\", \"sum\": \"some\", \"dat\": \"that\", \"dis\": \"this\", \"dem\": \"them\", \"dey\": \"they\", \"em\": \"them\", \"whered\": \"where did\", \"whod\": \"who did\", \"whos\": \"who's\", \"yall\": \"you all\", \"dam\": \"damn\", \"hell\": \"hell\", \"pissed\": \"pissed\", \"fck\": \"fuck\", \"fuk\": \"fuck\", \"effing\": \"effing\", \"bs\": \"bullshit\", \"crap\": \"crap\", \"shitty\": \"shitty\", \"wtf\": \"what the fuck\", \"stfu\": \"shut the fuck up\", \"gtfo\": \"get the fuck out\", \"irl\": \"in real life\", \"roflmao\": \"rolling on the floor laughing my ass off\", \"smol\": \"small\", \"big oof\": \"huge mistake\", \"yeet\": \"throw something forcefully\", \"pog\": \"play of the game\", \"sus\": \"suspicious\", \"cap\": \"lie\", \"no cap\": \"no lie\", \"bet\": \"okay\", \"lit\": \"amazing\", \"fr\": \"for real\", \"bruh\": \"bro\", \"fam\": \"family\",\n",
    "                    \"goat\": \"greatest of all time\", \"lowkey\": \"somewhat\", \"highkey\": \"very\", \"vibe\": \"mood\", \"drip\": \"fashionable\", \"slay\": \"do something well\", \"tea\": \"gossip\", \"sksksk\": \"excited reaction\", \"tf\": \"the fuck\", \"fomo\": \"fear of missing out\", \"tldr\": \"too long didn't read\", \"hmu\": \"hit me up\", \"wyd\": \"what you doing\", \"wym\": \"what you mean\", \"wdym\": \"what do you mean\", \"btw\": \"by the way\", \"imo\": \"in my opinion\", \"idc\": \"i don't care\", \n",
    "                  \"idgaf\": \"i don't give a f***\", \"frfr\": \"for real for real\", \"mf\": \"motherfucker\", \"rn\": \"right now\", \"lolz\": \"laughs\", \"hmu\": \"hit me up\", \"hbu\": \"how about you\", \"rn\": \"right now\", \"lmk\": \"let me know\", \"ikr\": \"i know right\", \"jk\": \"just kidding\", \"fr\": \"for real\", \"ffs\": \"for fuck's sake\", \"btw\": \"by the way\", \"nfs\": \"not for sale\", \"dm\": \"direct message\", \"af\": \"as fuck\", \"idk\": \"i don't know\"}\n",
    "\n",
    "def correct_slangs(text, slang_dict):\n",
    "    for slang, replacement in slang_dict.items():\n",
    "        text = re.sub(rf'\\b{slang}\\b', replacement, text)\n",
    "    return text\n",
    "\n",
    "# Slang dÃ¼zeltmeyi orijinal review'e uygula (henÃ¼z temizlenmemiÅŸ)\n",
    "df['cleaned_review'] = df['cleaned_review'].apply(lambda x: correct_slangs(x.lower(), slang_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiketleri sayÄ±ya Ã§evir: 0 â†’ negatif, 4 â†’ pozitif\n",
    "df['label'] = df['sentiment'].map({0: 0, 4: 1})\n",
    "\n",
    "# NaN olanlarÄ± temizle (her ihtimale karÅŸÄ±)\n",
    "df = df.dropna(subset=['cleaned_review', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_sentiment140.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"cleaned_sentiment140.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"cleaned_sentiment140.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri ÅŸekilleri:\n",
      "Train: (1280000,), Test: (320000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ã–zellik ve etiketleri ayÄ±r\n",
    "X = df['cleaned_review'].values\n",
    "y = df['label'].values\n",
    "\n",
    "# %80 train, %20 test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Kontrol\n",
    "print(\"Veri ÅŸekilleri:\")\n",
    "print(f\"Train: {x_train.shape}, Test: {x_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# TfidfVectorizer (Binary + max 5000 feature)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "x_train_vec = vectorizer.fit_transform(x_train)\n",
    "x_test_vec = vectorizer.transform(x_test)\n",
    "\n",
    "# Model eÄŸitme ve deÄŸerlendirme fonksiyonu\n",
    "def train_and_evaluate_model(model, model_name):\n",
    "    print(f\"\\nğŸ”¹ {model_name}\")\n",
    "    model.fit(x_train_vec, y_train)\n",
    "    preds = model.predict(x_test_vec)\n",
    "    # Skorlar\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Logistic Regression\n",
      "Accuracy: 0.7903\n",
      "F1 Score: 0.7936\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_model(LogisticRegression(max_iter=1000), \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_model(KNeighborsClassifier(n_neighbors=3), \"K-Nearest Neighbors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "train_and_evaluate_model(RandomForestClassifier(n_estimators=100), \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "train_and_evaluate_model(MultinomialNB(), \"Multinomial Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Linear SVC\n",
      "Accuracy: 0.7898\n",
      "F1 Score: 0.7938\n"
     ]
    }
   ],
   "source": [
    "# Linear Support Vector Classifier (SVC'den Ã§ok daha hÄ±zlÄ±dÄ±r)\n",
    "from sklearn.svm import LinearSVC\n",
    "train_and_evaluate_model(LinearSVC(), \"Linear SVC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Tokenizer ayarlarÄ±\n",
    "max_words = 10000     # En sÄ±k kullanÄ±lan 10.000 kelime\n",
    "max_len = 200         # CÃ¼mle uzunluÄŸu sabitlenecek\n",
    "\n",
    "# Tokenizer eÄŸitimi\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "# Metinleri dizilere Ã§evir\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Pad (doldurma)\n",
    "x_train_pad = pad_sequences(x_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "x_test_pad = pad_sequences(x_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Etiketleri array yap\n",
    "y_train_pad = np.array(y_train)\n",
    "y_test_pad = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_model(model_type='rnn', \n",
    "                        embedding_dim=64, \n",
    "                        hidden_dim=64, \n",
    "                        dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Basit derin Ã¶ÄŸrenme modelleri oluÅŸturmak iÃ§in esnek fonksiyon.\n",
    "    -------------------------------------------------------------\n",
    "    model_type : 'rnn', 'lstm', 'gru', 'cnn'\n",
    "    embedding_dim : Embedding katmanÄ±ndaki vektÃ¶r boyutu\n",
    "    hidden_dim : RNN/GRU/LSTM katmanlarÄ±ndaki gizli boyut \n",
    "                 veya CNN Conv1D 'filters' sayÄ±sÄ±\n",
    "    dropout_rate : Katman sonlarÄ±nda Dropout oranÄ±\n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Embedding katmanÄ± (tÃ¼m modeller iÃ§in ortak)\n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "        input_dim=max_words,     # Global deÄŸiÅŸken, Ã¶rn. 10000\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_len     # Global deÄŸiÅŸken, Ã¶rn. 200\n",
    "    ))\n",
    "\n",
    "    # Tek katmanlÄ± RNN/LSTM/GRU/CNN\n",
    "    if model_type == 'rnn':\n",
    "        model.add(tf.keras.layers.SimpleRNN(hidden_dim, dropout=dropout_rate))\n",
    "    elif model_type == 'lstm':\n",
    "        model.add(tf.keras.layers.LSTM(hidden_dim, dropout=dropout_rate))\n",
    "    elif model_type == 'gru':\n",
    "        model.add(tf.keras.layers.GRU(hidden_dim, dropout=dropout_rate))\n",
    "    elif model_type == 'cnn':\n",
    "        model.add(tf.keras.layers.Conv1D(\n",
    "            filters=hidden_dim,\n",
    "            kernel_size=3,\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be one of: 'rnn', 'lstm', 'gru', 'cnn'\")\n",
    "\n",
    "    # Opsiyonel Dropout (son katmandan sonra)\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Tek Dense katmanÄ±\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    # Ã‡Ä±kÄ±ÅŸ\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Model derleme\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_and_evaluate(model_type):\n",
    "    print(f\"\\nğŸ“š Training {model_type.upper()} model...\")\n",
    "\n",
    "    # Modeli oluÅŸtur\n",
    "    model = create_model(model_type)\n",
    "\n",
    "    # En iyi modeli kaydetmek iÃ§in callback\n",
    "    # 'val_accuracy' metrikte en iyi (maksimum) deÄŸeri gÃ¶rdÃ¼ÄŸÃ¼nde kaydeder\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f'best_{model_type}.keras',  # Dosya adÄ±\n",
    "        monitor='val_accuracy',           # izlenecek metrik\n",
    "        mode='max',                           # en yÃ¼ksek metrik deÄŸerini seÃ§\n",
    "        save_best_only=True,             # sadece en iyi modeli sakla\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # EÄŸitim\n",
    "    history = model.fit(\n",
    "        x_train_pad, y_train,\n",
    "        epochs=5,\n",
    "        batch_size=128,\n",
    "        validation_split=0.2,\n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint_callback]   # callback listesi\n",
    "    )\n",
    "\n",
    "    # EÄŸitim bittiÄŸinde 'best_{model_type}.h5' dosyasÄ±nda en iyi epoch modeli var\n",
    "    # Åimdi o en iyi modeli yÃ¼kleyelim\n",
    "    best_model = tf.keras.models.load_model(f'best_{model_type}.keras')\n",
    "\n",
    "    # Test verisinde tahmin\n",
    "    y_pred = (best_model.predict(x_test_pad) > 0.5).astype(\"int32\")\n",
    "\n",
    "    # Rapor\n",
    "    print(f\"\\nğŸ“Š {model_type.upper()} Classification Report (Best Epoch):\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“š Training CNN model...\n",
      "Epoch 1/5\n",
      "\u001b[1m7962/7962\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7592 - loss: 0.4887\n",
      "Epoch 1: val_accuracy improved from -inf to 0.79129, saving model to best_cnn.keras\n",
      "\u001b[1m7962/7962\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 55ms/step - accuracy: 0.7592 - loss: 0.4887 - val_accuracy: 0.7913 - val_loss: 0.4444\n",
      "Epoch 2/5\n",
      "\u001b[1m7961/7962\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8016 - loss: 0.4266\n",
      "Epoch 2: val_accuracy improved from 0.79129 to 0.79489, saving model to best_cnn.keras\n",
      "\u001b[1m7962/7962\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 46ms/step - accuracy: 0.8016 - loss: 0.4266 - val_accuracy: 0.7949 - val_loss: 0.4379\n",
      "Epoch 3/5\n",
      "\u001b[1m7961/7962\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8148 - loss: 0.4028\n",
      "Epoch 3: val_accuracy did not improve from 0.79489\n",
      "\u001b[1m7962/7962\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 46ms/step - accuracy: 0.8148 - loss: 0.4028 - val_accuracy: 0.7939 - val_loss: 0.4428\n",
      "Epoch 4/5\n",
      "\u001b[1m7961/7962\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8287 - loss: 0.3773\n",
      "Epoch 4: val_accuracy did not improve from 0.79489\n",
      "\u001b[1m7962/7962\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 45ms/step - accuracy: 0.8287 - loss: 0.3773 - val_accuracy: 0.7910 - val_loss: 0.4510\n",
      "Epoch 5/5\n",
      "\u001b[1m7962/7962\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8421 - loss: 0.3528\n",
      "Epoch 5: val_accuracy did not improve from 0.79489\n",
      "\u001b[1m7962/7962\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 48ms/step - accuracy: 0.8421 - loss: 0.3528 - val_accuracy: 0.7851 - val_loss: 0.4688\n",
      "\u001b[1m9952/9952\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5ms/step\n",
      "\n",
      "ğŸ“Š CNN Classification Report (Best Epoch):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79    158611\n",
      "           1       0.79      0.80      0.80    159838\n",
      "\n",
      "    accuracy                           0.79    318449\n",
      "   macro avg       0.79      0.79      0.79    318449\n",
      "weighted avg       0.79      0.79      0.79    318449\n",
      "\n",
      "\n",
      "ğŸ“š Training LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sagla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3651/7962\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m42:56\u001b[0m 598ms/step - accuracy: 0.4993 - loss: 0.6933"
     ]
    }
   ],
   "source": [
    "for m in ['cnn', 'lstm','gru']:\n",
    "    train_and_evaluate(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
