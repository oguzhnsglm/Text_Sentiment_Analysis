{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sagla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sagla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sagla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sagla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re, string\n",
    "import nltk\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Sadece gerekli olan NLTK kaynaklarını indiriyoruz\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                             review\n",
      "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1          0  is upset that he can't update his Facebook by ...\n",
      "2          0  @Kenichan I dived many times for the ball. Man...\n",
      "3          0    my whole body feels itchy and like its on fire \n",
      "4          0  @nationwideclass no, it's not behaving at all....\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV dosyasını yükle\n",
    "df = pd.read_csv(\"sentiment140_dataset.csv\", header=None, encoding='latin-1')\n",
    "\n",
    "# Orijinal sütunlara geçici isimler verelim\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'review']\n",
    "\n",
    "# Sadece sentiment ve review sütunlarını al\n",
    "df = df[['sentiment', 'review']]\n",
    "\n",
    "# sentiment sütununu string'den integer'a çevir\n",
    "df['sentiment'] = df['sentiment'].astype(int)\n",
    "\n",
    "# İlk 5 satırı kontrol et\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_basic(text):\n",
    "    # Küçük harf yap\n",
    "    text = text.lower()\n",
    "    \n",
    "    # HTML etiketlerini kaldır\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # URL'leri kaldır\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # 🔹 Mentionları kaldır (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Noktalama işaretlerini kaldır\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Emoji temizleme\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\" u\"\\U0001F600-\\U0001F64F\" u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\" u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002702-\\U000027B0\" \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # Fazla boşlukları temizle\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# POS bilgisi olmadan varsayılan isim (noun) kabul ederek sade lemmatization\n",
    "def lemmatize_simple(text):\n",
    "    words = text.split()  # burada tokenizer yerine split kullanıyoruz\n",
    "    cleaned = [\n",
    "        lemmatizer.lemmatize(word) for word in words\n",
    "        if word.lower() not in stop_words and word.isalpha()\n",
    "    ]\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "# Full temizlik + lemmatizasyon\n",
    "def full_clean(text):\n",
    "    text = clean_text_basic(text)\n",
    "    text = lemmatize_simple(text)\n",
    "    return text\n",
    "\n",
    "# Uygula\n",
    "df['cleaned_review'] = df['review'].apply(full_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = {\"omg\": \"oh my god\", \"lol\": \"laugh out loud\", \"u\": \"you\", \"r\": \"are\", \"gr8\": \"great\", \"b4\": \"before\", \"asap\": \"as soon as possible\", \"btw\": \"\", \"fyi\": \"for your information\", \"idk\": \"i don't know\", \"imo\": \"in my opinion\", \"jk\": \"just kidding\", \"lmao\": \"laughing my ass off\", \"lmk\": \"let me know\", \"nvm\": \"nevermind\", \"np\": \"no problem\", \"rofl\": \"rolling on the floor laughing\", \"smh\": \"shaking my head\", \"tbh\": \"to be honest\", \"thx\": \"thanks\", \"ty\": \"thank you\", \"wth\": \"what the heck\", \"wtf\": \"what the f***\", \"yolo\": \"you only live once\", \"brb\": \"be right back\", \"gtg\": \"got to go\", \"btw\": \"by the way\", \"ttyl\": \"talk to you later\", \"ily\": \"I love you\", \"g2g\": \"got to go\", \"h8\": \"hate\", \"jk\": \"just kidding\", \"thx\": \"thanks\", \"ttyl\": \"talk to you later\", \"cya\": \"see you\", \"gg\": \"good game\", \"afk\": \"away from keyboard\", \"ez\": \"easy\", \"wb\": \"welcome back\", \"idc\": \"i don't care\", \"rn\": \"right now\", \"lmk\": \"let me know\", \"ikr\": \"i know right\", \"tmi\": \"too much information\", \"smh\": \"shaking my head\", \"w/e\": \"whatever\", \n",
    "                 \"bff\": \"best friends forever\", \"tfw\": \"that feeling when\", \"np\": \"no problem\", \"nvm\": \"nevermind\", \"fyi\": \"for your information\", \"cuz\": \"because\", \"gonna\": \"going to\", \"wanna\": \"want to\", \"gotta\": \"got to\", \"kinda\": \"kind of\", \"sorta\": \"sort of\", \"dunno\": \"don't know\", \"ain't\": \"is not\", \"gimme\": \"give me\", \"lemme\": \"let me\", \"im\": \"I'm\", \"hes\": \"he's\", \"shes\": \"she's\", \"theyre\": \"they're\", \"youre\": \"you're\", \"havent\": \"have not\", \"cant\": \"cannot\",\n",
    "                  \"couldnt\": \"could not\", \"didnt\": \"did not\", \"doesnt\": \"does not\", \"dont\": \"do not\", \"isnt\": \"is not\", \"mightnt\": \"might not\", \"mustnt\": \"must not\", \"shouldnt\": \"should not\", \"wasnt\": \"was not\", \"werent\": \"were not\", \"wouldnt\": \"would not\", \"tho\": \"though\", \"thru\": \"through\", \"nite\": \"night\", \"neva\": \"never\", \"sum\": \"some\", \"dat\": \"that\", \"dis\": \"this\", \"dem\": \"them\", \"dey\": \"they\", \"em\": \"them\", \"whered\": \"where did\", \"whod\": \"who did\", \"whos\": \"who's\", \"yall\": \"you all\", \"dam\": \"damn\", \"hell\": \"hell\", \"pissed\": \"pissed\", \"fck\": \"fuck\", \"fuk\": \"fuck\", \"effing\": \"effing\", \"bs\": \"bullshit\", \"crap\": \"crap\", \"shitty\": \"shitty\", \"wtf\": \"what the fuck\", \"stfu\": \"shut the fuck up\", \"gtfo\": \"get the fuck out\", \"irl\": \"in real life\", \"roflmao\": \"rolling on the floor laughing my ass off\", \"smol\": \"small\", \"big oof\": \"huge mistake\", \"yeet\": \"throw something forcefully\", \"pog\": \"play of the game\", \"sus\": \"suspicious\", \"cap\": \"lie\", \"no cap\": \"no lie\", \"bet\": \"okay\", \"lit\": \"amazing\", \"fr\": \"for real\", \"bruh\": \"bro\", \"fam\": \"family\",\n",
    "                    \"goat\": \"greatest of all time\", \"lowkey\": \"somewhat\", \"highkey\": \"very\", \"vibe\": \"mood\", \"drip\": \"fashionable\", \"slay\": \"do something well\", \"tea\": \"gossip\", \"sksksk\": \"excited reaction\", \"tf\": \"the fuck\", \"fomo\": \"fear of missing out\", \"tldr\": \"too long didn't read\", \"hmu\": \"hit me up\", \"wyd\": \"what you doing\", \"wym\": \"what you mean\", \"wdym\": \"what do you mean\", \"btw\": \"by the way\", \"imo\": \"in my opinion\", \"idc\": \"i don't care\", \n",
    "                  \"idgaf\": \"i don't give a f***\", \"frfr\": \"for real for real\", \"mf\": \"motherfucker\", \"rn\": \"right now\", \"lolz\": \"laughs\", \"hmu\": \"hit me up\", \"hbu\": \"how about you\", \"rn\": \"right now\", \"lmk\": \"let me know\", \"ikr\": \"i know right\", \"jk\": \"just kidding\", \"fr\": \"for real\", \"ffs\": \"for fuck's sake\", \"btw\": \"by the way\", \"nfs\": \"not for sale\", \"dm\": \"direct message\", \"af\": \"as fuck\", \"idk\": \"i don't know\"}\n",
    "\n",
    "def correct_slangs(text, slang_dict):\n",
    "    for slang, replacement in slang_dict.items():\n",
    "        text = re.sub(rf'\\b{slang}\\b', replacement, text)\n",
    "    return text\n",
    "\n",
    "# Slang düzeltmeyi orijinal review'e uygula (henüz temizlenmemiş)\n",
    "df['cleaned_review'] = df['cleaned_review'].apply(lambda x: correct_slangs(x.lower(), slang_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiketleri sayıya çevir: 0 → negatif, 4 → pozitif\n",
    "df['label'] = df['sentiment'].map({0: 0, 4: 1})\n",
    "\n",
    "# NaN olanları temizle (her ihtimale karşı)\n",
    "df = df.dropna(subset=['cleaned_review', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_sentiment140.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"cleaned_sentiment140.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"cleaned_sentiment140.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri şekilleri:\n",
      "Train: (1280000,), Test: (320000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Özellik ve etiketleri ayır\n",
    "X = df['cleaned_review'].values\n",
    "y = df['label'].values\n",
    "\n",
    "# %80 train, %20 test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Kontrol\n",
    "print(\"Veri şekilleri:\")\n",
    "print(f\"Train: {x_train.shape}, Test: {x_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# TfidfVectorizer (Binary + max 5000 feature)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "x_train_vec = vectorizer.fit_transform(x_train)\n",
    "x_test_vec = vectorizer.transform(x_test)\n",
    "\n",
    "# Model eğitme ve değerlendirme fonksiyonu\n",
    "def train_and_evaluate_model(model, model_name):\n",
    "    print(f\"\\n🔹 {model_name}\")\n",
    "    model.fit(x_train_vec, y_train)\n",
    "    preds = model.predict(x_test_vec)\n",
    "    # Skorlar\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Logistic Regression\n",
      "Accuracy: 0.7903\n",
      "F1 Score: 0.7936\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_model(LogisticRegression(max_iter=1000), \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_model(KNeighborsClassifier(n_neighbors=3), \"K-Nearest Neighbors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "train_and_evaluate_model(RandomForestClassifier(n_estimators=100), \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "train_and_evaluate_model(MultinomialNB(), \"Multinomial Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Linear SVC\n",
      "Accuracy: 0.7898\n",
      "F1 Score: 0.7938\n"
     ]
    }
   ],
   "source": [
    "# Linear Support Vector Classifier (SVC'den çok daha hızlıdır)\n",
    "from sklearn.svm import LinearSVC\n",
    "train_and_evaluate_model(LinearSVC(), \"Linear SVC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Tokenizer ayarları\n",
    "max_words = 10000     # En sık kullanılan 10.000 kelime\n",
    "max_len = 200         # Cümle uzunluğu sabitlenecek\n",
    "\n",
    "# Tokenizer eğitimi\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "# Metinleri dizilere çevir\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Pad (doldurma)\n",
    "x_train_pad = pad_sequences(x_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "x_test_pad = pad_sequences(x_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Etiketleri array yap\n",
    "y_train_pad = np.array(y_train)\n",
    "y_test_pad = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_model(model_type='rnn', \n",
    "                        embedding_dim=64, \n",
    "                        hidden_dim=64, \n",
    "                        dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Basit derin öğrenme modelleri oluşturmak için esnek fonksiyon.\n",
    "    -------------------------------------------------------------\n",
    "    model_type : 'rnn', 'lstm', 'gru', 'cnn'\n",
    "    embedding_dim : Embedding katmanındaki vektör boyutu\n",
    "    hidden_dim : RNN/GRU/LSTM katmanlarındaki gizli boyut \n",
    "                 veya CNN Conv1D 'filters' sayısı\n",
    "    dropout_rate : Katman sonlarında Dropout oranı\n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Embedding katmanı (tüm modeller için ortak)\n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "        input_dim=max_words,     # Global değişken, örn. 10000\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_len     # Global değişken, örn. 200\n",
    "    ))\n",
    "\n",
    "    # Tek katmanlı RNN/LSTM/GRU/CNN\n",
    "    if model_type == 'rnn':\n",
    "        model.add(tf.keras.layers.SimpleRNN(hidden_dim, dropout=dropout_rate))\n",
    "    elif model_type == 'lstm':\n",
    "        model.add(tf.keras.layers.LSTM(hidden_dim, dropout=dropout_rate))\n",
    "    elif model_type == 'gru':\n",
    "        model.add(tf.keras.layers.GRU(hidden_dim, dropout=dropout_rate))\n",
    "    elif model_type == 'cnn':\n",
    "        model.add(tf.keras.layers.Conv1D(\n",
    "            filters=hidden_dim,\n",
    "            kernel_size=3,\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be one of: 'rnn', 'lstm', 'gru', 'cnn'\")\n",
    "\n",
    "    # Opsiyonel Dropout (son katmandan sonra)\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Tek Dense katmanı\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    # Çıkış\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Model derleme\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_and_evaluate(model_type):\n",
    "    print(f\"\\n📚 Training {model_type.upper()} model...\")\n",
    "\n",
    "    # Modeli oluştur\n",
    "    model = create_model(model_type)\n",
    "\n",
    "    # En iyi modeli kaydetmek için callback\n",
    "    # 'val_accuracy' metrikte en iyi (maksimum) değeri gördüğünde kaydeder\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f'best_{model_type}.keras',  # Dosya adı\n",
    "        monitor='val_accuracy',           # izlenecek metrik\n",
    "        mode='max',                           # en yüksek metrik değerini seç\n",
    "        save_best_only=True,             # sadece en iyi modeli sakla\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Eğitim\n",
    "    history = model.fit(\n",
    "        x_train_pad, y_train,\n",
    "        epochs=5,\n",
    "        batch_size=128,\n",
    "        validation_split=0.2,\n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint_callback]   # callback listesi\n",
    "    )\n",
    "\n",
    "    # Eğitim bittiğinde 'best_{model_type}.h5' dosyasında en iyi epoch modeli var\n",
    "    # Şimdi o en iyi modeli yükleyelim\n",
    "    best_model = tf.keras.models.load_model(f'best_{model_type}.keras')\n",
    "\n",
    "    # Test verisinde tahmin\n",
    "    y_pred = (best_model.predict(x_test_pad) > 0.5).astype(\"int32\")\n",
    "\n",
    "    # Rapor\n",
    "    print(f\"\\n📊 {model_type.upper()} Classification Report (Best Epoch):\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📚 Training CNN model...\n",
      "Epoch 1/5\n",
      "\u001b[1m7962/7962\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7592 - loss: 0.4887\n",
      "Epoch 1: val_accuracy improved from -inf to 0.79129, saving model to best_cnn.keras\n",
      "\u001b[1m7962/7962\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 55ms/step - accuracy: 0.7592 - loss: 0.4887 - val_accuracy: 0.7913 - val_loss: 0.4444\n",
      "Epoch 2/5\n",
      "\u001b[1m7961/7962\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8016 - loss: 0.4266\n",
      "Epoch 2: val_accuracy improved from 0.79129 to 0.79489, saving model to best_cnn.keras\n",
      "\u001b[1m7962/7962\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 46ms/step - accuracy: 0.8016 - loss: 0.4266 - val_accuracy: 0.7949 - val_loss: 0.4379\n",
      "Epoch 3/5\n",
      "\u001b[1m7961/7962\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8148 - loss: 0.4028\n",
      "Epoch 3: val_accuracy did not improve from 0.79489\n",
      "\u001b[1m7962/7962\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 46ms/step - accuracy: 0.8148 - loss: 0.4028 - val_accuracy: 0.7939 - val_loss: 0.4428\n",
      "Epoch 4/5\n",
      "\u001b[1m7961/7962\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8287 - loss: 0.3773\n",
      "Epoch 4: val_accuracy did not improve from 0.79489\n",
      "\u001b[1m7962/7962\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 45ms/step - accuracy: 0.8287 - loss: 0.3773 - val_accuracy: 0.7910 - val_loss: 0.4510\n",
      "Epoch 5/5\n",
      "\u001b[1m7962/7962\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8421 - loss: 0.3528\n",
      "Epoch 5: val_accuracy did not improve from 0.79489\n",
      "\u001b[1m7962/7962\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 48ms/step - accuracy: 0.8421 - loss: 0.3528 - val_accuracy: 0.7851 - val_loss: 0.4688\n",
      "\u001b[1m9952/9952\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5ms/step\n",
      "\n",
      "📊 CNN Classification Report (Best Epoch):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79    158611\n",
      "           1       0.79      0.80      0.80    159838\n",
      "\n",
      "    accuracy                           0.79    318449\n",
      "   macro avg       0.79      0.79      0.79    318449\n",
      "weighted avg       0.79      0.79      0.79    318449\n",
      "\n",
      "\n",
      "📚 Training LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sagla\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3651/7962\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m42:56\u001b[0m 598ms/step - accuracy: 0.4993 - loss: 0.6933"
     ]
    }
   ],
   "source": [
    "for m in ['cnn', 'lstm','gru']:\n",
    "    train_and_evaluate(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
